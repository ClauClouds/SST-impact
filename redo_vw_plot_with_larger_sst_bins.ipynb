{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61f945be-8888-40dd-9ce0-922fb049c5c6",
   "metadata": {},
   "source": [
    "rebinning of the vertical velocity data on a new sst grid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74636fce-5bd6-4fb5-8ff5-39da27378cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary libraries\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from datetime import datetime\n",
    "import matplotlib.dates as mdates\n",
    "import glob\n",
    "#from myFunctions import lcl\n",
    "#from myFunctions import f_closest\n",
    "from warnings import warn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import interpolate\n",
    "import custom_color_palette as ccp\n",
    "from matplotlib import rcParams\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "\n",
    "def f_interpolate_SST_and_merge(SST_DS, dataset_obs):\n",
    "    '''function to interpolate SST values on the time resolution of the observations given as input\n",
    "    input: \n",
    "    SST_DS: xarray dataset containing sst values\n",
    "    dataset_obs: xarray dataset containing the observations to merge with sst data\n",
    "    output: \n",
    "    data_merged: data returned \n",
    "    '''\n",
    "    \n",
    "    # interpolating sst data at 1 min resolution to the 10 s res of the wind lidar\n",
    "    sst_data_interp = SST_DS.interp(time=dataset_obs['time'].values)\n",
    "\n",
    "    # merging the interpolated dataset and the wind lidar dataset\n",
    "    data_merged = xr.merge([dataset_obs, sst_data_interp])\n",
    "    return(data_merged)\n",
    "\n",
    "\n",
    "def f_calculate_binned_data(data_input, SST_binned_arr):\n",
    "    \n",
    "    ''' function to calculate mean values of all variables for each SST bin, for all instruments\n",
    "    author: Claudia Acquistapace\n",
    "    date: 20 Sept 2021\n",
    "    input: - data_input: input xarray dataset containing the variables as a function of time, height, to be averaged\n",
    "            - SST_binned_arr: numpy array of sst binned values for calculating the mean \n",
    "    output: dataset_concat: xarray dataset of concatenated values with mean profiles corresponding to the sst bins. A variable n_el counts the number of profiles averaged together\n",
    "    '''\n",
    "    # calculating mean quantities f\n",
    "    dataset_mean = []\n",
    "\n",
    "    data_input = data_input.load()\n",
    "\n",
    "    # selecting all columns in the bin interval\n",
    "    for ind_bin in range(len(SST_binned_arr)-1):\n",
    "\n",
    "        # selecting slices of datasets columns with SST values in the selected bin\n",
    "        DS_sliced = data_merged.where((data_input.SST > SST_binned_arr[ind_bin]) & (data_input.SST < SST_binned_arr[ind_bin+1]), drop=True)\n",
    "\n",
    "        # add variable of the number of elements of the slice\n",
    "        n_el = len(DS_sliced.SST.values)\n",
    "        DS_sliced['n_elements'] = n_el\n",
    "\n",
    "        # calculate mean profile averaging all selected time stamps together\n",
    "        dataset_mean.append(DS_sliced.mean(dim='time', skipna=True))\n",
    "\n",
    "\n",
    "    # concatenating datasets corresponding to SST bins on a new bin dimension\n",
    "    dataset_concat = xr.concat([dataset_mean[i] for i in np.arange(len(dataset_mean))], dim='SST_binned')\n",
    "    return(dataset_concat)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dict_plot_settings = {\n",
    "    'labelsizeaxes':14,\n",
    "    'fontSizeTitle':16,\n",
    "    'fontSizeX'    :16,\n",
    "    'fontSizeY'    :16,\n",
    "    'cbarAspect'   :10,\n",
    "    'fontSizeCbar' :16,\n",
    "    'rcparams_font':['Tahoma'],\n",
    "    'savefig_dpi'  :100,\n",
    "    'font_size'    :22, \n",
    "    'grid'         :True}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data_path = '/Volumes/Extreme SSD/work/006_projects/001_Prec_Trade_Cycle/post_processed_data/diurnal_cycle_removed/'\n",
    "tsg_file = \"/Volumes/Extreme SSD/work/006_projects/001_Prec_Trade_Cycle/tsg_sst_data/tsg/nc/msm_089_1_tsg.nc\"\n",
    "path_out_plots = '/Volumes/Extreme SSD/work/006_projects/001_Prec_Trade_Cycle/SST_impact_work/plots_paper/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "493da333-b3f4-4996-a2c4-e19545892a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading tsg file ( data with 1 min resolution)\n",
    "tsg_data = xr.open_dataset(tsg_file)\n",
    "\n",
    "# reading data containing flags to filter out rainy columns\n",
    "flag_file_list = np.sort(glob.glob('/Volumes/Extreme SSD/work/006_projects/001_Prec_Trade_Cycle/post_processed_data/*_flags_cloud_properties.nc'))\n",
    "flag_file_list = flag_file_list[13:15]\n",
    "\n",
    "flag_data = xr.open_mfdataset(flag_file_list)\n",
    "\n",
    "# reading tsg file ( data with 1 min resolution)\n",
    "tsg_data = xr.open_dataset(tsg_file)\n",
    "\n",
    "# identifying time stamps of sst corresponding to time stamps of radiosondes\n",
    "t_start = datetime(2020, 2, 2, 0, 0, 0)\n",
    "t_end = datetime(2020, 2, 3, 23, 59, 59)\n",
    "\n",
    "# slicing tsg datase t for the selected time interval and extracting sst\n",
    "sliced_tsg_ds = tsg_data.sel(TIME=slice(t_start, t_end))\n",
    "tsg_sst = sliced_tsg_ds['TEMP'].values\n",
    "tsg_time_sst = sliced_tsg_ds['TIME'].values\n",
    "tsg_flag = sliced_tsg_ds['TEMP_QC'].values\n",
    "\n",
    "# averaging together the sst of the different sst sensors for tsg\n",
    "temp0 = sliced_tsg_ds.TEMP[:,0].values\n",
    "temp1 = sliced_tsg_ds.TEMP[:,1].values\n",
    "sst_tsg = temp0\n",
    "sst_tsg[np.isnan(temp0)] = temp1[np.isnan(temp0)]\n",
    "\n",
    "# producing output dataset of sst_tsg for the selected time window\n",
    "# creating dataset with coordinates sst and height\n",
    "dim_sst           = ['time']\n",
    "coords         = {\"time\":sliced_tsg_ds.TIME.values}\n",
    "SST              = xr.DataArray(dims=dim_sst, coords=coords, data=sst_tsg,\n",
    "                 attrs={'long_name':'sea surface temperature ',\n",
    "                        'units':'$^{\\circ}$C'})\n",
    "variables         = {'SST':SST}\n",
    "SST_DS      = xr.Dataset(data_vars = variables,\n",
    "                       coords = coords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef5a6879-b3c7-4839-adcb-f759a7dccd0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[26.283 26.308 26.333 26.358 26.383 26.408 26.433 26.458 26.483 26.508\n",
      " 26.533 26.558 26.583 26.608 26.633 26.658 26.683 26.708 26.733 26.758\n",
      " 26.783 26.808 26.833 26.858 26.883 26.908 26.933 26.958 26.983 27.008\n",
      " 27.033 27.058 27.083 27.108 27.133 27.158 27.183 27.208 27.233 27.258\n",
      " 27.283 27.308 27.333 27.358 27.383 27.408 27.433 27.458 27.483 27.508\n",
      " 27.533 27.558 27.583 27.608 27.633 27.658]\n",
      "0\n",
      "26.283\n",
      "2\n",
      "26.333\n",
      "4\n",
      "26.382999999999996\n",
      "6\n",
      "26.432999999999993\n",
      "8\n",
      "26.48299999999999\n",
      "10\n",
      "26.532999999999987\n",
      "12\n",
      "26.582999999999984\n",
      "14\n",
      "26.63299999999998\n",
      "16\n",
      "26.68299999999998\n",
      "18\n",
      "26.732999999999976\n",
      "20\n",
      "26.782999999999973\n",
      "22\n",
      "26.83299999999997\n",
      "24\n",
      "26.882999999999967\n",
      "26\n",
      "26.932999999999964\n",
      "28\n",
      "26.98299999999996\n",
      "30\n",
      "27.03299999999996\n",
      "32\n",
      "27.082999999999956\n",
      "34\n",
      "27.132999999999953\n",
      "36\n",
      "27.18299999999995\n",
      "38\n",
      "27.232999999999947\n",
      "40\n",
      "27.282999999999944\n",
      "42\n",
      "27.33299999999994\n",
      "44\n",
      "27.38299999999994\n",
      "46\n",
      "27.432999999999936\n",
      "48\n",
      "27.482999999999933\n",
      "50\n",
      "27.53299999999993\n",
      "52\n",
      "27.582999999999927\n",
      "54\n",
      "27.632999999999925\n",
      "[26.283 26.333 26.383 26.433 26.483 26.533 26.583 26.633 26.683 26.733\n",
      " 26.783 26.833 26.883 26.933 26.983 27.033 27.083 27.133 27.183 27.233\n",
      " 27.283 27.333 27.383 27.433 27.483 27.533 27.583 27.633  0.   ]\n"
     ]
    }
   ],
   "source": [
    "# building SST binned array\n",
    "SST_min = np.nanmin(sst_tsg)\n",
    "SST_max = np.nanmax(sst_tsg)\n",
    "SST_binned_arr = np.arange(SST_min, SST_max, step=0.025)\n",
    "\n",
    "\n",
    "\n",
    "# define new sst array for reducing the resolution of the sampling\n",
    "def f_calc_new_sst(sst_arr):\n",
    "    \n",
    "    print(sst_arr)\n",
    "    new_sst_arr = np.zeros(int(len(sst_arr)/2)+1)\n",
    "    ind_new = 0\n",
    "    for ind_sst in range(len(sst_arr)):\n",
    "        if (ind_sst % 2) == 0:\n",
    "            print(ind_sst)\n",
    "            print(sst_arr[ind_sst])\n",
    "            new_sst_arr[ind_new] = sst_arr[ind_sst]\n",
    "            ind_new = ind_new+1\n",
    "    print(new_sst_arr)\n",
    "    return(new_sst_arr)\n",
    "new_sst_arr =  f_calc_new_sst(SST_binned_arr)\n",
    "\n",
    "\n",
    "# calculate label marks for bins\n",
    "sst_bin_label = []\n",
    "for ind in range(len(new_sst_arr)-1):\n",
    "    sst_bin_label.append(round((new_sst_arr[ind]+new_sst_arr[ind+1])/2,2))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b27497b6-f7b1-4617-8ad5-4a085f9de061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing variable:  Vertical velocity\n",
      "/Volumes/Extreme SSD/work/006_projects/001_Prec_Trade_Cycle/post_processed_data/diurnal_cycle_removed/VW*.nc\n",
      "['/Volumes/Extreme SSD/work/006_projects/001_Prec_Trade_Cycle/post_processed_data/diurnal_cycle_removed/VW_20200202.nc'\n",
      " '/Volumes/Extreme SSD/work/006_projects/001_Prec_Trade_Cycle/post_processed_data/diurnal_cycle_removed/VW_20200203.nc']\n",
      "files read\n",
      "flag interpolated\n",
      "mask calculated\n",
      "<xarray.Dataset>\n",
      "Dimensions:                   (height: 57, time: 16617)\n",
      "Coordinates:\n",
      "  * time                      (time) datetime64[ns] 2020-02-02T00:00:08 ... 2...\n",
      "  * height                    (height) float32 225.0 275.0 ... 2975.0 3025.0\n",
      "Data variables:\n",
      "    product_no_diurnal_cycle  (time, height) float64 dask.array<chunksize=(8438, 57), meta=np.ndarray>\n",
      "    product_no_noise          (time, height) float64 dask.array<chunksize=(8438, 57), meta=np.ndarray>\n",
      "    nans                      (time, height) float64 dask.array<chunksize=(8438, 57), meta=np.ndarray>\n",
      "    SST                       (time) float64 nan nan nan ... 26.98 26.98 26.98\n",
      "interpolation and merging done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/claudia/opt/anaconda3/lib/python3.8/site-packages/dask/array/numpy_compat.py:39: RuntimeWarning: invalid value encountered in true_divide\n",
      "  x = np.divide(x1, x2, out)\n"
     ]
    }
   ],
   "source": [
    "# reading all arthus data \n",
    "\n",
    "# variable list\n",
    "var_list =['VW']\n",
    "\n",
    "VW_dict = {\n",
    "     'var_name'  : 'VW',\n",
    "     'var_string': 'Vertical velocity',\n",
    "     'var_units' : ' ms$^{-1}$',\n",
    "     'var_min'   : -2.,\n",
    "     'var_max'   : 2.,\n",
    "     'thr_min'   : -5.,\n",
    "     'thr_max'   : 5.,\n",
    "     'avg_time'  : '15',\n",
    "     'cmap'      : 'seismic',\n",
    "     'title'     : 'Vertical velocity: 28/01-04/02'}\n",
    "\n",
    "\n",
    "dict_list = [VW_dict]\n",
    "#dict_list = [theta_dict, theta_e_dict]\n",
    "\n",
    "\n",
    "vars_arthus = []\n",
    "\n",
    "for i_var, dict_var in enumerate(dict_list):\n",
    "\n",
    "    print('processing variable: ', dict_var['var_string'])\n",
    "    print(data_path+dict_var['var_name']+'*.nc')\n",
    "    \n",
    "    # reading file list of the files for the selected variable\n",
    "    arthus_file_list = np.sort(glob.glob(data_path+dict_var['var_name']+'_2020*.nc'))\n",
    "    print(arthus_file_list)\n",
    "    \n",
    "    # read the two datasets together\n",
    "    arthus_dataset = xr.open_mfdataset(arthus_file_list)\n",
    "    \n",
    "    # renaming variable time and height and dimension time and height (step necessary for next operations)\n",
    "    #arthus_dataset = arthus_dataset.rename_dims({'Time':'time'})\n",
    "    #arthus_dataset = arthus_dataset.rename_dims({'Height':'height'})\n",
    "    #arthus_dataset = arthus_dataset.rename_vars({'Time':'time'})\n",
    "    #arthus_dataset = arthus_dataset.rename_vars({'Height':'height'})\n",
    "\n",
    "    print('files read')\n",
    "\n",
    "    # interpolate flag on time resolution of arthus data, picking the closest time stamp to lidar time stamps\n",
    "    flag_data_interp = flag_data.interp(time=arthus_dataset['time'].values, method='nearest')\n",
    "\n",
    "    print('flag interpolated')\n",
    "    \n",
    "    # building a mask to filter out Ze rainy columns and substitute them with nans\n",
    "    # set to nan the values out of the thresholds for the selected variable\n",
    "    mask = np.zeros((len(arthus_dataset.time.values), len(arthus_dataset.height.values)))\n",
    "    for ind in range(len(flag_data_interp.time.values)):\n",
    "        if (flag_data_interp[\"flag_rain_ground\"].values[ind] == 1) | (flag_data_interp[\"flag_rain\"].values[ind] == 1):\n",
    "            mask[ind,:] = np.repeat(1, len(arthus_dataset.height.values))\n",
    "\n",
    "    print('mask calculated')\n",
    "    \n",
    "    arthus_dataset[\"nans\"] = xr.full_like(arthus_dataset.product_no_diurnal_cycle, fill_value=np.nan)\n",
    "    arthus_dataset['product_no_diurnal_cycle'] = xr.where(mask == 0, arthus_dataset['product_no_diurnal_cycle'], arthus_dataset[\"nans\"])\n",
    "\n",
    "    \n",
    "    # interpolating SST data on the arthus data \n",
    "    arthus_all_SST = f_interpolate_SST_and_merge(SST_DS, arthus_dataset)\n",
    "    print(arthus_all_SST)\n",
    "    \n",
    "    \n",
    "    \n",
    "    print('interpolation and merging done')\n",
    "\n",
    "    # calculating mean quantities for each bin\n",
    "    dataset_mean = []\n",
    "    dataset_std = []\n",
    "    dataset_n = []\n",
    "    \n",
    "    # selecting all columns in the bin interval\n",
    "    for ind_bin in range(len(new_sst_arr)-1):\n",
    "\n",
    "        # selecting slices of datasets columns with SST values in the selected bin\n",
    "        DS_sliced = arthus_all_SST.where((arthus_all_SST.SST > new_sst_arr[ind_bin]) & (arthus_all_SST.SST < new_sst_arr[ind_bin+1]), drop=True)\n",
    "\n",
    "        # add variable of the number of elements of the slice\n",
    "        n_el = len(DS_sliced.SST.values)\n",
    "        \n",
    "        # add variable of absolute value of the var\n",
    "        dims_ds = ['time','height']\n",
    "        coords_ds = {'time':DS_sliced['time'].values, 'height':DS_sliced['height'].values}\n",
    "        abs_val = np.abs(DS_sliced['product_no_diurnal_cycle'].values)\n",
    "        DS_sliced['abs_val'] =  xr.DataArray(dims=dims_ds, coords=coords_ds, data=abs_val)\n",
    "\n",
    "    \n",
    "        # add variable of count of values for each sliced dataset where we calculate mean/std \n",
    "        dims_ds = ['time','height']\n",
    "        coords_ds = {'time':DS_sliced['time'].values, 'height':DS_sliced['height'].values}\n",
    "        counts = np.count_nonzero(~np.isnan(DS_sliced['product_no_diurnal_cycle'].values), axis=0)\n",
    "        #print(np.shape(counts))\n",
    "        #print(counts)\n",
    "        DS_sliced['n_elements'] =  xr.DataArray(dims=['height'], \\\n",
    "                                                coords={'height':DS_sliced['height'].values}, \\\n",
    "                                                data=counts, \n",
    "                                               attrs={'long_name':'number of values in each bin SST/height'})        \n",
    "        \n",
    "        \n",
    "        # calculate mean profile averaging all selected time stamps together\n",
    "        dataset_mean.append(DS_sliced.mean(dim='time', skipna=True))\n",
    "        dataset_std.append(DS_sliced.std(dim='time', skipna=True))\n",
    "        dataset_n.append(DS_sliced['n_elements'])\n",
    "        \n",
    "        \n",
    "    # concatenating datasets corresponding to SST bins on a new bin dimension\n",
    "    arthus_SST_concat = xr.concat([dataset_mean[i] for i in np.arange(len(dataset_mean))], dim='SST_binned')\n",
    "    arthus_SST_std_concat = xr.concat([dataset_std[i] for i in np.arange(len(dataset_std))], dim='SST_binned')\n",
    "    arthus_SST_n_concat = xr.concat([dataset_n[i] for i in np.arange(len(dataset_n))], dim='SST_binned')\n",
    "    \n",
    "    \n",
    "    # saving variable of interest in a dictionary with its name: for vertical velocity (VW) we save the absolute value\n",
    "    # of vertical velocity for mean variable, and the std of the vertical velocity values.\n",
    "    if dict_var['var_name']!= 'VW':\n",
    "        dict_variable_nodc = {'var_name':dict_var['var_name'],\n",
    "                     'var':arthus_SST_concat['product_no_diurnal_cycle'].values, \n",
    "                     'std':arthus_SST_std_concat['product_no_diurnal_cycle'].values,\n",
    "                      'n':arthus_SST_n_concat.data}\n",
    "    else:\n",
    "        dict_variable_nodc = {'var_name':dict_var['var_name'],\n",
    "                     'var':arthus_SST_concat['abs_val'].values, \n",
    "                     'std':arthus_SST_std_concat['product_no_diurnal_cycle'].values,\n",
    "                      'n':arthus_SST_n_concat.data}       \n",
    "    \n",
    "    \n",
    "    # append the dictionary in a list of dictionaries containing all arthus variables\n",
    "    vars_arthus.append(dict_variable_nodc)\n",
    "\n",
    "\n",
    "# converting the list of dictionaries in a dictionary called variables to be saved in ncdf\n",
    "dims             = ['sst','height']\n",
    "coords           = {'sst':new_sst_arr[0:-1], 'height':arthus_SST_concat['height'].values}\n",
    "variables = {}\n",
    "for i in range(len(dict_list)):\n",
    "    key = vars_arthus[i]['var_name']\n",
    "    value = xr.DataArray(dims=dims, coords=coords, data=vars_arthus[i]['var'],\n",
    "                             attrs={'long_name':vars_arthus[i]['var_name']+' without diurnal cycle',\n",
    "                                    'units':dict_var['var_units']})\n",
    "    value_std = xr.DataArray(dims=dims, coords=coords, data=vars_arthus[i]['std'],\n",
    "                             attrs={'long_name':'std of '+vars_arthus[i]['var_name']+' without diurnal cycle',\n",
    "                                    'units':dict_var['var_units']})\n",
    "    value_n = xr.DataArray(dims=dims, coords=coords, data=vars_arthus[i]['n'],\n",
    "                             attrs={'long_name':'number of '+vars_arthus[i]['var_name']+' values in the bin',\n",
    "                                    'units':'#'})\n",
    "    variables[key] = value\n",
    "    variables[key+'_std'] = value_std\n",
    "    variables[key+'_n'] = value_n\n",
    "\n",
    "\n",
    "global_attributes = {'CREATED_BY'       : 'Claudia Acquistapace',\n",
    "                        'CREATED_ON'       :  str(datetime.now()),\n",
    "                        'FILL_VALUE'       :  'NaN', \n",
    "                        'PI_NAME'          : 'Claudia Acquistapace',\n",
    "                        'PI_AFFILIATION'   : 'University of Cologne (UNI), Germany', \n",
    "                        'PI_ADDRESS'       : 'Institute for geophysics and meteorology, Pohligstrasse 3, 50969 Koeln', \n",
    "                        'PI_MAIL'          : 'cacquist@meteo.uni-koeln.de',\n",
    "                        'DATA_DESCRIPTION' : dict_var['var_string']+'with the diurnal cycle removed from the data',\n",
    "                        'DATA_DISCIPLINE'  : 'Atmospheric Physics - Remote Sensing Lidar Profiler',\n",
    "                        'DATA_GROUP'       : 'Experimental;Profile;Moving',\n",
    "                        'DATA_SOURCE'      : 'arthus data',\n",
    "                        'DATA_PROCESSING'  : 'https://github.com/ClauClouds/SST-impact/',\n",
    "                        'INSTRUMENT_MODEL' : 'arthus raman lidar system',\n",
    "                         'COMMENT'         : 'original data postprocessed by Diego Lange' }\n",
    "dataset_out    = xr.Dataset(data_vars = variables,\n",
    "                        coords = coords,\n",
    "                        attrs = global_attributes)\n",
    "dataset_out.to_netcdf('/Volumes/Extreme SSD/work/006_projects/001_Prec_Trade_Cycle/post_processed_data/binned_sst/arthus_binned_WV_coarser_sst.nc')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b5e495-1dea-4a43-8fb8-fc2af85c0ff5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
