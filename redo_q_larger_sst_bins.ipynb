{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b621510d-0131-4fff-b923-e196d0986a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary libraries\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from datetime import datetime\n",
    "import matplotlib.dates as mdates\n",
    "import glob\n",
    "#from myFunctions import lcl\n",
    "#from myFunctions import f_closest\n",
    "from warnings import warn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import interpolate\n",
    "import custom_color_palette as ccp\n",
    "from matplotlib import rcParams\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "\n",
    "def f_interpolate_SST_and_merge(SST_DS, dataset_obs):\n",
    "    '''function to interpolate SST values on the time resolution of the observations given as input\n",
    "    input: \n",
    "    SST_DS: xarray dataset containing sst values\n",
    "    dataset_obs: xarray dataset containing the observations to merge with sst data\n",
    "    output: \n",
    "    data_merged: data returned \n",
    "    '''\n",
    "    \n",
    "    # interpolating sst data at 1 min resolution to the 10 s res of the wind lidar\n",
    "    sst_data_interp = SST_DS.interp(time=dataset_obs['time'].values)\n",
    "\n",
    "    # merging the interpolated dataset and the wind lidar dataset\n",
    "    data_merged = xr.merge([dataset_obs, sst_data_interp])\n",
    "    return(data_merged)\n",
    "\n",
    "\n",
    "def f_calculate_binned_data(data_input, SST_binned_arr):\n",
    "    \n",
    "    ''' function to calculate mean values of all variables for each SST bin, for all instruments\n",
    "    author: Claudia Acquistapace\n",
    "    date: 20 Sept 2021\n",
    "    input: - data_input: input xarray dataset containing the variables as a function of time, height, to be averaged\n",
    "            - SST_binned_arr: numpy array of sst binned values for calculating the mean \n",
    "    output: dataset_concat: xarray dataset of concatenated values with mean profiles corresponding to the sst bins. A variable n_el counts the number of profiles averaged together\n",
    "    '''\n",
    "    # calculating mean quantities f\n",
    "    dataset_mean = []\n",
    "\n",
    "    data_input = data_input.load()\n",
    "\n",
    "    # selecting all columns in the bin interval\n",
    "    for ind_bin in range(len(SST_binned_arr)-1):\n",
    "\n",
    "        # selecting slices of datasets columns with SST values in the selected bin\n",
    "        DS_sliced = data_merged.where((data_input.SST > SST_binned_arr[ind_bin]) & (data_input.SST < SST_binned_arr[ind_bin+1]), drop=True)\n",
    "\n",
    "        # add variable of the number of elements of the slice\n",
    "        n_el = len(DS_sliced.SST.values)\n",
    "        DS_sliced['n_elements'] = n_el\n",
    "\n",
    "        # calculate mean profile averaging all selected time stamps together\n",
    "        dataset_mean.append(DS_sliced.mean(dim='time', skipna=True))\n",
    "\n",
    "\n",
    "    # concatenating datasets corresponding to SST bins on a new bin dimension\n",
    "    dataset_concat = xr.concat([dataset_mean[i] for i in np.arange(len(dataset_mean))], dim='SST_binned')\n",
    "    return(dataset_concat)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dict_plot_settings = {\n",
    "    'labelsizeaxes':14,\n",
    "    'fontSizeTitle':16,\n",
    "    'fontSizeX'    :16,\n",
    "    'fontSizeY'    :16,\n",
    "    'cbarAspect'   :10,\n",
    "    'fontSizeCbar' :16,\n",
    "    'rcparams_font':['Tahoma'],\n",
    "    'savefig_dpi'  :100,\n",
    "    'font_size'    :22, \n",
    "    'grid'         :True}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data_path = '/Volumes/Extreme SSD/work/006_projects/001_Prec_Trade_Cycle/post_processed_data/diurnal_cycle_removed/'\n",
    "tsg_file = \"/Volumes/Extreme SSD/work/006_projects/001_Prec_Trade_Cycle/tsg_sst_data/tsg/nc/msm_089_1_tsg.nc\"\n",
    "path_out_plots = '/Volumes/Extreme SSD/work/006_projects/001_Prec_Trade_Cycle/SST_impact_work/plots_paper/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c44e081-7c8c-4868-9a82-75c12e904813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading tsg file ( data with 1 min resolution)\n",
    "tsg_data = xr.open_dataset(tsg_file)\n",
    "\n",
    "# reading data containing flags to filter out rainy columns\n",
    "flag_file_list = np.sort(glob.glob('/Volumes/Extreme SSD/work/006_projects/001_Prec_Trade_Cycle/post_processed_data/*_flags_cloud_properties.nc'))\n",
    "flag_file_list = flag_file_list[13:15]\n",
    "\n",
    "flag_data = xr.open_mfdataset(flag_file_list)\n",
    "\n",
    "# reading tsg file ( data with 1 min resolution)\n",
    "tsg_data = xr.open_dataset(tsg_file)\n",
    "\n",
    "# identifying time stamps of sst corresponding to time stamps of radiosondes\n",
    "t_start = datetime(2020, 2, 2, 0, 0, 0)\n",
    "t_end = datetime(2020, 2, 3, 23, 59, 59)\n",
    "\n",
    "# slicing tsg datase t for the selected time interval and extracting sst\n",
    "sliced_tsg_ds = tsg_data.sel(TIME=slice(t_start, t_end))\n",
    "tsg_sst = sliced_tsg_ds['TEMP'].values\n",
    "tsg_time_sst = sliced_tsg_ds['TIME'].values\n",
    "tsg_flag = sliced_tsg_ds['TEMP_QC'].values\n",
    "\n",
    "# averaging together the sst of the different sst sensors for tsg\n",
    "temp0 = sliced_tsg_ds.TEMP[:,0].values\n",
    "temp1 = sliced_tsg_ds.TEMP[:,1].values\n",
    "sst_tsg = temp0\n",
    "sst_tsg[np.isnan(temp0)] = temp1[np.isnan(temp0)]\n",
    "\n",
    "# producing output dataset of sst_tsg for the selected time window\n",
    "# creating dataset with coordinates sst and height\n",
    "dim_sst           = ['time']\n",
    "coords         = {\"time\":sliced_tsg_ds.TIME.values}\n",
    "SST              = xr.DataArray(dims=dim_sst, coords=coords, data=sst_tsg,\n",
    "                 attrs={'long_name':'sea surface temperature ',\n",
    "                        'units':'$^{\\circ}$C'})\n",
    "variables         = {'SST':SST}\n",
    "SST_DS      = xr.Dataset(data_vars = variables,\n",
    "                       coords = coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8a98702-b0a3-4067-819e-9ddf2af2e601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# building SST binned array\n",
    "#SST_min = np.nanmin(sst_tsg)\n",
    "#SST_max = np.nanmax(sst_tsg)\n",
    "#SST_binned_arr = np.arange(SST_min, SST_max, step=0.025)\n",
    "\n",
    "SST_binned_arr = np.asarray([26.2 , 26.6 , 26.8 , 27.1 , 27.3 , 27.66])\n",
    "\n",
    "\n",
    "# calculate label marks for bins\n",
    "sst_bin_label = []\n",
    "for ind in range(len(SST_binned_arr)-1):\n",
    "    sst_bin_label.append(round((SST_binned_arr[ind]+SST_binned_arr[ind+1])/2,2))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "546b1887-3245-4249-aa7d-5d241ff51c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing variable:  Water vapor mixing ratio\n",
      "/Volumes/Extreme SSD/work/006_projects/001_Prec_Trade_Cycle/post_processed_data/diurnal_cycle_removed/MR*.nc\n",
      "['/Volumes/Extreme SSD/work/006_projects/001_Prec_Trade_Cycle/post_processed_data/diurnal_cycle_removed/MR_20200202.nc'\n",
      " '/Volumes/Extreme SSD/work/006_projects/001_Prec_Trade_Cycle/post_processed_data/diurnal_cycle_removed/MR_20200203.nc']\n",
      "files read\n",
      "flag interpolated\n",
      "mask calculated\n",
      "<xarray.Dataset>\n",
      "Dimensions:                   (height: 57, time: 16617)\n",
      "Coordinates:\n",
      "  * time                      (time) datetime64[ns] 2020-02-02T00:00:08 ... 2...\n",
      "  * height                    (height) float32 225.0 275.0 ... 2975.0 3025.0\n",
      "Data variables:\n",
      "    product_no_diurnal_cycle  (time, height) float64 dask.array<chunksize=(8438, 57), meta=np.ndarray>\n",
      "    product_no_noise          (time, height) float64 dask.array<chunksize=(8438, 57), meta=np.ndarray>\n",
      "    nans                      (time, height) float64 dask.array<chunksize=(8438, 57), meta=np.ndarray>\n",
      "    SST                       (time) float64 nan nan nan ... 26.98 26.98 26.98\n",
      "interpolation and merging done\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'strasuka' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/vq/pfv4n2796kvc782gmx0q4bhc0000gn/T/ipykernel_4343/3131751089.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0marthus_all_SST\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_netcdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/Volumes/Extreme SSD/work/006_projects/001_Prec_Trade_Cycle/post_processed_data/binned_sst/arthus_all_MR.nc'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'interpolation and merging done'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m     \u001b[0mstrasuka\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m     \u001b[0;31m# calculating mean quantities for each bin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0mdataset_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'strasuka' is not defined"
     ]
    }
   ],
   "source": [
    "# reading all arthus data \n",
    "\n",
    "# variable list\n",
    "var_list =['MR']\n",
    "\n",
    "WVMR_dict = {\n",
    "     'var_name'  : 'MR',\n",
    "     'var_string': 'Water vapor mixing ratio',\n",
    "     'var_units' : ' g kg$^{-1}$',\n",
    "     'var_min'   : 0.,\n",
    "     'var_max'   : 30.,\n",
    "     'thr_min'   : 0.,\n",
    "     'thr_max'   : 30.,\n",
    "     'avg_time'  : '15',\n",
    "     'cmap'      : 'jet',\n",
    "     'title'     : 'Water vapor mixing ratio: 28/01-04/02'}\n",
    "\n",
    "\n",
    "dict_list = [WVMR_dict]\n",
    "#dict_list = [theta_dict, theta_e_dict]\n",
    "\n",
    "\n",
    "vars_arthus = []\n",
    "\n",
    "for i_var, dict_var in enumerate(dict_list):\n",
    "\n",
    "    print('processing variable: ', dict_var['var_string'])\n",
    "    print(data_path+dict_var['var_name']+'*.nc')\n",
    "    \n",
    "    # reading file list of the files for the selected variable\n",
    "    arthus_file_list = np.sort(glob.glob(data_path+dict_var['var_name']+'_2020*.nc'))\n",
    "    print(arthus_file_list)\n",
    "    \n",
    "    # read the two datasets together\n",
    "    arthus_dataset = xr.open_mfdataset(arthus_file_list)\n",
    "    \n",
    "    # renaming variable time and height and dimension time and height (step necessary for next operations)\n",
    "    #arthus_dataset = arthus_dataset.rename_dims({'Time':'time'})\n",
    "    #arthus_dataset = arthus_dataset.rename_dims({'Height':'height'})\n",
    "    #arthus_dataset = arthus_dataset.rename_vars({'Time':'time'})\n",
    "    #arthus_dataset = arthus_dataset.rename_vars({'Height':'height'})\n",
    "\n",
    "    print('files read')\n",
    "\n",
    "    # interpolate flag on time resolution of arthus data, picking the closest time stamp to lidar time stamps\n",
    "    flag_data_interp = flag_data.interp(time=arthus_dataset['time'].values, method='nearest')\n",
    "\n",
    "    print('flag interpolated')\n",
    "    \n",
    "    # building a mask to filter out Ze rainy columns and substitute them with nans\n",
    "    # set to nan the values out of the thresholds for the selected variable\n",
    "    mask = np.zeros((len(arthus_dataset.time.values), len(arthus_dataset.height.values)))\n",
    "    for ind in range(len(flag_data_interp.time.values)):\n",
    "        if (flag_data_interp[\"flag_rain_ground\"].values[ind] == 1) | (flag_data_interp[\"flag_rain\"].values[ind] == 1):\n",
    "            mask[ind,:] = np.repeat(1, len(arthus_dataset.height.values))\n",
    "\n",
    "    print('mask calculated')\n",
    "    \n",
    "    arthus_dataset[\"nans\"] = xr.full_like(arthus_dataset.product_no_diurnal_cycle, fill_value=np.nan)\n",
    "    arthus_dataset['product_no_diurnal_cycle'] = xr.where(mask == 0, arthus_dataset['product_no_diurnal_cycle'], arthus_dataset[\"nans\"])\n",
    "\n",
    "    \n",
    "    # interpolating SST data on the arthus data \n",
    "    arthus_all_SST = f_interpolate_SST_and_merge(SST_DS, arthus_dataset)\n",
    "    print(arthus_all_SST)\n",
    "    \n",
    "    # save arthus data to ncdf for calculating MLQ on all data\n",
    "    arthus_all_SST.to_netcdf('/Volumes/Extreme SSD/work/006_projects/001_Prec_Trade_Cycle/post_processed_data/binned_sst/arthus_all_MR.nc')\n",
    "    print('interpolation and merging done')\n",
    "    strasuka\n",
    "    # calculating mean quantities for each bin\n",
    "    dataset_mean = []\n",
    "    dataset_std = []\n",
    "    dataset_n = []\n",
    "    \n",
    "    # selecting all columns in the bin interval\n",
    "    for ind_bin in range(len(SST_binned_arr)-1):\n",
    "\n",
    "        # selecting slices of datasets columns with SST values in the selected bin\n",
    "        DS_sliced = arthus_all_SST.where((arthus_all_SST.SST > SST_binned_arr[ind_bin]) & (arthus_all_SST.SST < SST_binned_arr[ind_bin+1]), drop=True)\n",
    "    \n",
    "        # add variable of the number of elements of the slice\n",
    "        n_el = len(DS_sliced.SST.values)\n",
    "        \n",
    "        # add variable of absolute value of the var\n",
    "        dims_ds = ['time','height']\n",
    "        coords_ds = {'time':DS_sliced['time'].values, 'height':DS_sliced['height'].values}\n",
    "        abs_val = np.abs(DS_sliced['product_no_diurnal_cycle'].values)\n",
    "        val_no_noise = DS_sliced['product_no_noise'].values\n",
    "        \n",
    "        DS_sliced['val_no_noise'] = xr.DataArray(dims=dims_ds, coords=coords_ds, data=val_no_noise)\n",
    "        DS_sliced['abs_val'] =  xr.DataArray(dims=dims_ds, coords=coords_ds, data=abs_val)\n",
    "\n",
    "    \n",
    "        # add variable of count of values for each sliced dataset where we calculate mean/std \n",
    "        dims_ds = ['time','height']\n",
    "        coords_ds = {'time':DS_sliced['time'].values, 'height':DS_sliced['height'].values}\n",
    "        counts = np.count_nonzero(~np.isnan(DS_sliced['product_no_diurnal_cycle'].values), axis=0)\n",
    "        #print(np.shape(counts))\n",
    "        #print(counts)\n",
    "        DS_sliced['n_elements'] =  xr.DataArray(dims=['height'], \\\n",
    "                                                coords={'height':DS_sliced['height'].values}, \\\n",
    "                                                data=counts, \n",
    "                                               attrs={'long_name':'number of values in each bin SST/height'})        \n",
    "        \n",
    "        \n",
    "        # calculate mean profile averaging all selected time stamps together\n",
    "        dataset_mean.append(DS_sliced.mean(dim='time', skipna=True))\n",
    "        dataset_std.append(DS_sliced.std(dim='time', skipna=True))\n",
    "        dataset_n.append(DS_sliced['n_elements'])\n",
    "        \n",
    "        \n",
    "    # concatenating datasets corresponding to SST bins on a new bin dimension\n",
    "    arthus_SST_concat = xr.concat([dataset_mean[i] for i in np.arange(len(dataset_mean))], dim='SST_binned')\n",
    "    arthus_SST_std_concat = xr.concat([dataset_std[i] for i in np.arange(len(dataset_std))], dim='SST_binned')\n",
    "    arthus_SST_n_concat = xr.concat([dataset_n[i] for i in np.arange(len(dataset_n))], dim='SST_binned')\n",
    "    \n",
    "    \n",
    "    # saving variable of interest in a dictionary with its name: for vertical velocity (VW) we save the absolute value\n",
    "    # of vertical velocity for mean variable, and the std of the vertical velocity values.\n",
    "    if dict_var['var_name']!= 'VW':\n",
    "        dict_variable_nodc = {'var_name':dict_var['var_name'],\n",
    "                      'var_no_noise':arthus_SST_concat['val_no_noise'].values,\n",
    "                     'var':arthus_SST_concat['product_no_diurnal_cycle'].values, \n",
    "                     'std':arthus_SST_std_concat['product_no_diurnal_cycle'].values,\n",
    "                      'n':arthus_SST_n_concat.data}\n",
    "    else:\n",
    "        dict_variable_nodc = {'var_name':dict_var['var_name'],\n",
    "                     'var_no_noise':arthus_SST_concat['val_no_noise'].values,\n",
    "                     'var':arthus_SST_concat['abs_val'].values, \n",
    "                     'std':arthus_SST_std_concat['product_no_diurnal_cycle'].values,\n",
    "                      'n':arthus_SST_n_concat.data}       \n",
    "    \n",
    "    \n",
    "    # append the dictionary in a list of dictionaries containing all arthus variables\n",
    "    vars_arthus.append(dict_variable_nodc)\n",
    "\n",
    "\n",
    "# converting the list of dictionaries in a dictionary called variables to be saved in ncdf\n",
    "dims             = ['sst','height']\n",
    "coords           = {'sst':SST_binned_arr[0:-1], 'height':arthus_SST_concat['height'].values}\n",
    "variables = {}\n",
    "for i in range(len(dict_list)):\n",
    "    key = vars_arthus[i]['var_name']\n",
    "    value_no_noise =  xr.DataArray(dims=dims, coords=coords, data=vars_arthus[i]['var_no_noise'],\n",
    "                             attrs={'long_name':vars_arthus[i]['var_name']+' without noise',\n",
    "                                    'units':dict_var['var_units']})\n",
    "    value = xr.DataArray(dims=dims, coords=coords, data=vars_arthus[i]['var'],\n",
    "                             attrs={'long_name':vars_arthus[i]['var_name']+' without diurnal cycle',\n",
    "                                    'units':dict_var['var_units']})\n",
    "    value_std = xr.DataArray(dims=dims, coords=coords, data=vars_arthus[i]['std'],\n",
    "                             attrs={'long_name':'std of '+vars_arthus[i]['var_name']+' without diurnal cycle',\n",
    "                                    'units':dict_var['var_units']})\n",
    "    value_n = xr.DataArray(dims=dims, coords=coords, data=vars_arthus[i]['n'],\n",
    "                             attrs={'long_name':'number of '+vars_arthus[i]['var_name']+' values in the bin',\n",
    "                                    'units':'#'})\n",
    "    variables[key] = value\n",
    "    variables[key+'_std'] = value_std\n",
    "    variables[key+'_n'] = value_n\n",
    "    variables[key+'_no_noise'] = value_no_noise\n",
    "\n",
    "\n",
    "global_attributes = {'CREATED_BY'       : 'Claudia Acquistapace',\n",
    "                        'CREATED_ON'       :  str(datetime.now()),\n",
    "                        'FILL_VALUE'       :  'NaN', \n",
    "                        'PI_NAME'          : 'Claudia Acquistapace',\n",
    "                        'PI_AFFILIATION'   : 'University of Cologne (UNI), Germany', \n",
    "                        'PI_ADDRESS'       : 'Institute for geophysics and meteorology, Pohligstrasse 3, 50969 Koeln', \n",
    "                        'PI_MAIL'          : 'cacquist@meteo.uni-koeln.de',\n",
    "                        'DATA_DESCRIPTION' : dict_var['var_string']+'with the diurnal cycle removed from the data',\n",
    "                        'DATA_DISCIPLINE'  : 'Atmospheric Physics - Remote Sensing Lidar Profiler',\n",
    "                        'DATA_GROUP'       : 'Experimental;Profile;Moving',\n",
    "                        'DATA_SOURCE'      : 'arthus data',\n",
    "                        'DATA_PROCESSING'  : 'https://github.com/ClauClouds/SST-impact/',\n",
    "                        'INSTRUMENT_MODEL' : 'arthus raman lidar system',\n",
    "                         'COMMENT'         : 'original data postprocessed by Diego Lange' }\n",
    "dataset_out    = xr.Dataset(data_vars = variables,\n",
    "                        coords = coords,\n",
    "                        attrs = global_attributes)\n",
    "dataset_out.to_netcdf('/Volumes/Extreme SSD/work/006_projects/001_Prec_Trade_Cycle/post_processed_data/binned_sst/arthus_binned_MR_coarser_sst.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45967bc-d76c-4c7f-8f23-fe2216ddb002",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset_out)\n",
    "# reading humidity profile without noise\n",
    "q = dataset_out.MR_no_noise.values\n",
    "np.shape(q)\n",
    "height = dataset_out.height.values\n",
    "sst = dataset_out.sst.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "03be7715-f8a5-4951-8d03-564b94f228ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.316087588618997\n",
      "675.0\n",
      "14.998766981052766\n",
      "825.0\n",
      "15.185235725639876\n",
      "775.0\n",
      "13.242379766229682\n",
      "875.0\n",
      "13.751688389915694\n",
      "925.0\n"
     ]
    }
   ],
   "source": [
    "print(dataset_out)\n",
    "# reading humidity profile without noise\n",
    "q = dataset_out.MR_no_noise.values\n",
    "np.shape(q)\n",
    "height = dataset_out.height.values\n",
    "sst = dataset_out.sst.values\n",
    "\n",
    "MLQ_arr_all = np.zeros((np.shape(q)[0]))\n",
    "for ind in range(np.shape(q)[0]):\n",
    "    \n",
    "    ind_h = 0\n",
    "    # find minimum height where there's a q obs in RS profiles\n",
    "    while(np.isnan(q[ind,ind_h]) * (ind_h < 3100)):\n",
    "        ind_h = ind_h+1\n",
    "    q0 = q[ind,ind_h]\n",
    "    print(q0)\n",
    "    \n",
    "    # find height at which q(z) < q[0]-1\n",
    "    qo_prof = np.zeros(len(q[ind,:]))\n",
    "    qo_prof.fill(q0-1.)\n",
    "    diff = abs(q[ind,:] - qo_prof)\n",
    "\n",
    "    #print((np.where(el.q.values < q0-1.)[0][0]))\n",
    "    MLQ_arr_all[ind] = height[np.nanargmin(diff)]\n",
    "    print(MLQ_arr_all[ind])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "024fb9aa-634a-4a55-8e6e-856676645677",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7fdcf6e5e3d0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(sst, MLQ_arr_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bc520ae-36da-4f74-ae18-c81d31ba11a9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sst' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/vq/pfv4n2796kvc782gmx0q4bhc0000gn/T/ipykernel_4343/4062157692.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sst' is not defined"
     ]
    }
   ],
   "source": [
    "sst\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4165d00b-f19f-42b3-928a-1003e76bcbd2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
